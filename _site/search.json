[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About Me",
    "section": "",
    "text": "Data Analyst | Marketing Analytics Professional"
  },
  {
    "objectID": "about.html#professional-journey",
    "href": "about.html#professional-journey",
    "title": "About Me",
    "section": "Professional Journey",
    "text": "Professional Journey\nMy inspiration for pursuing Analytics stemmed from my previous role as a Media Planner, I had opportunity with our Marketing Science team to work on a geo-lift experiment for a client and I was able to analyze the historical data and identify the control cities as well as test group and once the experiment was successful, I was given the opportunity to present and that moment made me realize this comes naturally to me and I want to pursue this. I was promoted to Media Planning Manager and requested to have analytic responsibilities as well. From there on, I felt the industry was constantly evolving and I need to keep up with it so I decided to pursue Masters in Business Analytics at Boston University.\nAfter completing the course, I feel like I have a better understanding of the full marketing execution workflow all the way upto analysis of campaign performance and reporting. Most importantly I am not afraid to challenge myself to learn new tools and techniques that can be used in Marketing Measurement frameworks. I have also had opportunity to work hands-on with healthcare data and there I am most interested in analyzing patient data for resource allocation, identifying trends/patterns of symptoms for a particular disease and even claims data to recommend ways of streamlining the process more efficiently. Over the period of last 1.5 years I have worked with classmates, teammembers on hackathons, professors on exploring advanced topics while preparing for job applications and what I have realized is that I have developed a practice of using AI to execute my logic and completely depend on my learning and understanding for figuring out how to solve business problems by using the data at hand. I have been constantly practicing converting business problems into analytic questions for the data so my query and code writing becomes easy.\nApart from practicing proficiency in usual analytical tools and techniques like Python, R, SQL and BI platforms like Power BI, Excel and Tableau, I am willing to learn tools that an organization requires, document processes and constantly work on improving my ability to communicate learnings from the data analysis in the most simple way so that coming to conclusions & business decisions is easier for stakeholders and decision makers."
  },
  {
    "objectID": "about.html#skills-expertise",
    "href": "about.html#skills-expertise",
    "title": "About Me",
    "section": "Skills & Expertise",
    "text": "Skills & Expertise\n\nTechnical Skills\n\nProgramming: Python, R\nDatabase: MySQL, PostgreSQL\nData Visualization: Tableau, Power BI, Excel, Datorama\nMachine Learning: Scikit-learn, Predictive Modeling, Regression algorithms, Forecasting models, Classification algorithms and Forecasting models\nCloud systems: Git, AWS, Spark (SparkSQL, PySpark)\nStatistics: Hypothesis testing, Chi-square test, T-tests, probability distributions\nMarketing Analytics: GA4 (Google Analytics), A/B testing, Media Mix Modeling (MMM), Market Research, Audience segmentation, Google Tag Manager (GTM), paid media strategy, Marketing measurement frameworks.\n\n\n\nDomain Expertise\n\nMarketing Analytics\nHealthcare Analytics\n\nBusiness Intelligence\nData Storytelling"
  },
  {
    "objectID": "about.html#education-certifications",
    "href": "about.html#education-certifications",
    "title": "About Me",
    "section": "Education & Certifications",
    "text": "Education & Certifications\n\nMasters of Science in Business Analytics - Boston University (2025)\nPostgraduate certificate in Brand Management - Seneca College of Arts & Science (2022)\nGoogle Data Analytics Professional Certificate - Google (2023)\nAWS Cloud Practitioner - Amazon Web Services (2025)\nMeta Certified Marketing Science Professional - Meta Blueprint (2025) Ongoing"
  },
  {
    "objectID": "about.html#currently-learning",
    "href": "about.html#currently-learning",
    "title": "About Me",
    "section": "Currently Learning",
    "text": "Currently Learning\n\nAdvanced Regression algorithms for Marketing Analytics.\nCore basics of Marketing Mix Modeling.\nAttribution and Incrementality testing & how to built these measurmenet frameworks for Marketing Analytics.\nUnderstanding pipelines and their design principles so I can make life easy for Marketing reporting teams.\n\n\n“Marketing without data analysis & research for a brand is like hoping to drive business results by random trials & opinions”"
  },
  {
    "objectID": "resume.html",
    "href": "resume.html",
    "title": "Professional Resume",
    "section": "",
    "text": "Now the headings are in Quarto markdown (not inside HTML blocks), so the TOC should work properly while still maintaining all your styling!"
  },
  {
    "objectID": "resume.html#professional-summary",
    "href": "resume.html#professional-summary",
    "title": "Professional Resume",
    "section": "Professional Summary",
    "text": "Professional Summary\n\nPaid media planner and analyst with over 2 years of experience in planning omni-channel media plans for clients across automotive, entertainment and insurance verticals. I have closely worked with Marketing Science team to establish my foundation in Marketing Analytics and had hands-on experience in designing experiments to report on incremental metrics for digital campaigns. My biggest achievement in my previous role was identifying audience segments for an automotive clients campaign using historical data and 3rd party audience data and then test it on new platforms & combining with influencer marketing to create a wider impact. The campaign was awarded Twitch campaign of the year for esports category in 2022 and I had the opportunity to present the case study.\nMore recently I am working as Data Analyst at a non-profit and using data from various government sources to empower their event planning and fundraising strategy using tools like SQL, Python and Power BI."
  },
  {
    "objectID": "resume.html#work-experience",
    "href": "resume.html#work-experience",
    "title": "Professional Resume",
    "section": "Work Experience",
    "text": "Work Experience\n\nData Analyst\nPropa City Community Outreach\nJune 2025 - Present\n\nEngineered a PostgreSQL relational database to centralize 1,500+ programs and resources records, reducing query times by 40% and improving data accessibility for monthly board reviews.\nAutomated Salesforce-to-database data ingestion using a Python ETL pipeline, reducing manual data collection and transfer time by 70%\nDeveloped a Power BI dashboard tracking annual events and maternal health program KPIs, enabling comprehensive reporting to board members and reallocation of $50K funding to workshops.\nPresented data-driven outreach insights during the Massachusetts PPD initiative, resulting in partnerships with 2 medical centres serving 3 neighborhoods.\n\n\n\nMedia Planning Manager\nOmnicom Media Group\nOct 2023 - Aug 2024\n\nDirected omni-channel planning for Cineplex Pan Canada using marketing mix modeling results, achieving a 20% YoY reach increase by reallocating 30% of Facebook budget to TikTok and CTV based on performance data\nLed annual media planning for Cineplex’s Playdium and Recroom brand portfolios of $3M, using Data Management Platforms (DMPs) audience segments as well as Adobe 1P data to shift 100% of TV budget to Programmatic CTV, increasing YoY reach by 20%\nOptimized Porsche’s Digital Plan by analyzing GA4 cross-channel attribution (paid social v/s display), reallocating 15% of Meta spend to high performing Twitch and Pinterest placements and combining it with Influencer boosted plan, winning ‘Twitch Esports Campaign of the year 2022.’\nCollaborated with Marketing science team to build a Datorama dashboard tracking cross-channel performance (paid social, search, programmatic, CTV & OOH), integrated platform-specific metrics (Facebook CPM, YouTube CPV) into a unified view, enabling weekly recommendations and optimization reports to the client.\nLed competitive analysis using industry and audience data from Telmar, Vividata, Nielsen and Environics through Omni, identifying Share of Voice (SOV) for Porsche’s luxury auto category.\n\n\n\nMedia Planning Associate\nOmnicom Media Group\nMay 2022 - Oct 2023\n\nManaged end-to-end ad operations including campaign trafficking sheets, tag QA, and insertion orders across multiple channels, reducing errors by 20% through systematic process improvements and cross-checks with execution team\nAssisted in QA checks for a Tableau dashboard (validating GA4 metrics vs. platform data) ensuring accuracy for bi-weekly client reporting presentations.\nPartnered with Digital execution team to leverage Lotame and Nielsen audiences under Omni for refining Ferrero’s ecommerce campaigns, improving ROAS by 3.5x than the traditional search plan.\nPulled and compiled weekly platform reports (Google Ads, DV360, Meta Ads Manager) for senior planners, flagging underperforming campaigns for optimization."
  },
  {
    "objectID": "resume.html#education",
    "href": "resume.html#education",
    "title": "Professional Resume",
    "section": "Education",
    "text": "Education\n\nMaster of Science in Applied Business Analytics\nBoston University\nExpected: May 2025 | GPA: 3.8/4.0\nRelevant Coursework: Data Mining, Big Data Analytics, Machine Learning, Data Visualization, Statistical Methods\n\n\nPost-graduate Certificate Diploma\nSeneca College of Arts & Science\nGraduated: April 2022 | GPA: 3.8/4.0\nRelevant Coursework: Case tudy analysis, Media planning, Advertising strategy, Product analysis."
  },
  {
    "objectID": "resume.html#technical-skills",
    "href": "resume.html#technical-skills",
    "title": "Resume",
    "section": "",
    "text": "Programming\nPython, R, SQL, JavaScript\n\n\nData Visualization\nTableau, Power BI, Matplotlib, Plotly\n\n\nMachine Learning\nScikit-learn, TensorFlow, Predictive Modeling\n\n\nTools & Platforms\nAWS, Git, Docker, Jupyter, Excel"
  },
  {
    "objectID": "resume.html#certifications",
    "href": "resume.html#certifications",
    "title": "Resume",
    "section": "",
    "text": "Google Data Analytics Professional Certificate - Google (2023)\nAWS Certified Cloud Practitioner - Amazon Web Services (2022)\nTableau Desktop Specialist - Tableau (2022)\n\n–"
  },
  {
    "objectID": "resume.html#projects",
    "href": "resume.html#projects",
    "title": "Resume",
    "section": "",
    "text": "Customer Segmentation Analysis: Clustered 50K+ customers using K-means\nSales Forecasting Model: Achieved 94% accuracy in quarterly predictions\nReal-time Dashboard: Built interactive dashboard monitoring key metrics"
  },
  {
    "objectID": "test-about.html",
    "href": "test-about.html",
    "title": "Test About",
    "section": "",
    "text": "Image Test\n\n\n\nIf you see a blue circle, the image loaded. If you see broken image icon, the URL is wrong."
  },
  {
    "objectID": "about.html#professional-journey-professional-journey-.content-section",
    "href": "about.html#professional-journey-professional-journey-.content-section",
    "title": "About Me",
    "section": "Professional Journey {#professional-journey .content-section",
    "text": "Professional Journey {#professional-journey .content-section\n[Your professional story here…]\nStarted my journey in data analytics when… Currently focused on… Passionate about…"
  },
  {
    "objectID": "about.html#skills-expertise-skills-expertise",
    "href": "about.html#skills-expertise-skills-expertise",
    "title": "About Me",
    "section": "Skills & Expertise {#skills-expertise",
    "text": "Skills & Expertise {#skills-expertise\n\nTechnical Skills\n\nProgramming: Python, R\nDatabase: MySQL, PostgreSQL\nData Visualization: Tableau, Power BI, Excel, Datorama\nMachine Learning: Scikit-learn, Predictive Modeling, Regression algorithms, Forecasting models, Classification algorithms and Forecasting models\nCloud systems: Git, AWS, Spark (SparkSQL, PySpark)\nStatistics: Hypothesis testing, Chi-square test, T-tests, probability distributions\nMarketing Analytics: GA4 (Google Analytics), A/B testing, Media Mix Modeling (MMM), Market Research, Audience segmentation, Google Tag Manager (GTM), paid media strategy, Marketing measurement frameworks.\n\n\n\nDomain Expertise\n\nMarketing Analytics\nHealthcare Analytics\n\nBusiness Intelligence\nData Storytelling"
  },
  {
    "objectID": "about.html#education-certifications-education-certifications-n",
    "href": "about.html#education-certifications-education-certifications-n",
    "title": "About Me",
    "section": "Education & Certifications {#education-certifications n}",
    "text": "Education & Certifications {#education-certifications n}\n\nBachelor of Science in Data Science - University Name (Year)\nGoogle Data Analytics Professional Certificate - Google (Year)\nAWS Cloud Practitioner - Amazon Web Services (Year)"
  },
  {
    "objectID": "about.html#skills",
    "href": "about.html#skills",
    "title": "About Me",
    "section": "Skills",
    "text": "Skills\n\nTechnical Skills\n\nProgramming: Python, R\nDatabase: MySQL, PostgreSQL\nData Visualization: Tableau, Power BI, Excel, Datorama\nMachine Learning: Scikit-learn, Predictive Modeling, Regression algorithms, Forecasting models, Classification algorithms and Forecasting models\nCloud systems: Git, AWS, Spark (SparkSQL, PySpark)\nStatistics: Hypothesis testing, Chi-square test, T-tests, probability distributions\nMarketing Analytics: GA4 (Google Analytics), A/B testing, Media Mix Modeling (MMM), Market Research, Audience segmentation, Google Tag Manager (GTM), paid media strategy, Marketing measurement frameworks.\n\n###Domain Expertise - Marketing Analytics - Healthcare Analytics\n- Business Intelligence - Data Storytelling\n##Education & Certifications {#education-certifications}\n\nBachelor of Science in Data Science - University Name (Year)\nGoogle Data Analytics Professional Certificate - Google (Year)\nAWS Cloud Practitioner - Amazon Web Services (Year)\n\n##Currently Learning {#currently-learning}\n\nAdvanced Machine Learning techniques\nCloud data engineering\nReal-time analytics platforms\n\n\n“Your favorite data-related quote or personal philosophy”"
  },
  {
    "objectID": "contact.html",
    "href": "contact.html",
    "title": "Contact form",
    "section": "",
    "text": "CONTACT ME\n\n\n\n  \n      \n      \n        \n          First Name\n          \n        \n        \n          Last Name\n          \n        \n      \n      \n      \n        Email\n        \n      \n      \n      \n        Type your message here...\n        \n      \n      \n      Submit\n    \n    \n  \n  \n  \n  \n    \n      Hiten Ladkani\n      Data Analyst | Marketing Analytics Professional\n    \n    \n    \n      \n        \n          \n        \n        Email\n      \n      \n      \n        \n          \n        \n        LinkedIn\n      \n      \n      \n        \n          \n        \n        GitHub"
  },
  {
    "objectID": "resume.html#technical-tools-stack",
    "href": "resume.html#technical-tools-stack",
    "title": "Professional Resume",
    "section": "Technical Tools Stack",
    "text": "Technical Tools Stack\n\n  \n    \n    Python\n  \n  \n    \n    SQL\n  \n  \n    \n    PostgreSQL\n  \n  \n    \n    R\n  \n  \n    \n    Apache Spark\n  \n  \n    \n    AWS EC2\n  \n  \n    \n    AWS S3\n  \n  \n    \n    VS Code\n  \n  \n    \n    Jupyter\n  \n  \n    \n    Google Colab\n  \n  \n    \n    GitHub\n  \n  \n    \n    Excel\n  \n  \n    \n    Power BI\n  \n  \n    \n    Tableau\n  \n  \n    \n    Looker Studio\n  \n  \n    \n    Google Analytics\n  \n  \n    \n    Quarto"
  },
  {
    "objectID": "projects.html",
    "href": "projects.html",
    "title": "Data Analytics Projects",
    "section": "",
    "text": "Ecommerce Sales Attribution and Marketing Segmentation\n\n  \n    \n      \n        \n      \n      \n        \n          Ecommerce Sales Attribution and Marketing Segmentation\n        \n        \n          Analyzing customer purchase patterns via an RFM model and marketing channel effectiveness to optimize sales attribution and create targeted customer segments for personalized marketing campaigns.\n        \n        \n          \n             GitHub\n          \n          \n             Tableau\n          \n        \n      \n    \n  \n\n  \n    \n      \n      \n        \n          Project Scenario\n        \n        \n          \n            Context and Overview\n            As a junior marketing data analyst for an e-commerce brand running on Shopify, my job is to analyze purchase and campaign data to identify what channels are driving most revenue, how different are purchase behaviors of various customer segments and if there are any untapped opportunities as well as how the business can improve further. \n            \n            Key Objectives\n            \n              Analyze sales performance metrics by channel for future budget allocation\n              Identify customer segment profiles and analyze their purchase patterns\n              Provide actionable recommendations for marketing channel mix and KPI improvements\n            \n          \n        \n      \n\n      \n      \n        \n          Business Problem and Stakeholders Questions\n        \n        \n          \n            Business Challenge\n            How can we clearly quantify each marketing channel impact on business outcomes and surface various customer segments for the brand by leveraging the Shopify transaction data as well as campaign data to optimize the marketing spend and personalize communication strategy for each customer segment? \n            \n            Stakeholder Questions\n            \n              E-commerce Manager: Needs clear performance metrics by each channel to formulate future budget allocation.\n              Marketing Lead: Seeks insight into attribution models (first-touch, last-touch and multi-touch) to optimize campaigns and channel mix.\n              Merchandising Team: Wants clearly identified customer segment profiles to tailor site content and promotions.\n            \n            \n          \n        \n      \n\n      \n      \n        \n          Data Requirements & Gathering\n        \n        \n          \n            Data Sources\n            \n              Orders table: This is the source table, and the table includes digital purchase transactions data. What was bought, when and how much. \n              DimAttribution: This is the dimension table which includes data on the traffic source in terms of marketing channel and which device the session was logged in\n              DimCustomer: Another dimension table that contains customer information. The orders table is the source of truth in this analysis, and the dimension tables will be joined to the source table to create our analytical base which will be the prep table.\n            \n            \n            Data Transformation & Consolidation\n            Prep Table - This table is prepared by using the RIGHT JOIN with the Orders table on the two dimension tables using order number as the reference key. \n            This provides attribution and customer information for each order number in the Orders table and those that have missing information are excluded. A quick data validation check is done to ensure that the prep table rows does not exceed order table rows in case of the RIGHT JOIN and in LEFT JOIN the row numbers should be equal.\n          \n        \n      \n\n      \n      \n        \n          Data Cleaning & Transformation\n        \n        \n          \n            Exploring distribution of key metrics and scope of data (Exploratory Data Analysis)\n             i. Summary statistics of Order & refund amounts:The order amounts distribution heavy right tail and most orders clustered on the left with higher skew on the left side because of refunded orders. At initial stage this suggests high number of one-time purchasers.\n            \n            \n            Summary statistics  \n\n             The refund amounts in the dataset is only 2, which reduces the scope of the analysis as we need not analyze these refund amounts since these numbers are expected in digital transactions.\n\n            \n            Summary statistics \n\n             ii. Scope of dataset and volume of orders by month:The data ranges from March 13th 2024 to 22nd July 2025, just over an year worth of data. \n\n            \n            Dataset scope \n\n            \n            Monthly orders \n\n            As typically observed in retail sales, correction happens after initial burst of sales after opening (assuming the brand opened its online store since March 2024) which has happened in this case as the sales in the 1st quarter of 2025 dropped to some extent. \n\n             iii. Analyzing distribution of key metrics:\n\n            \n            Order status distribution \n\n            Orders that have pending payment will be excluded from revenue and other analysis, so the results are not inflated. \n\n            \n            Payment methods distribution \n\n            Most of the transactions have been completed through card and some of these categories can be collapsed into one to streamline analysis.\n            \n            \n            Orders by countries \n\n            Similar to the trend in order amounts, majority of the transactions have come from 7-8 countries and the distribution is similar with heavy right tail.  \n\n            \n            Attribution Channels\n\n             Similar to payment methods, there are similar categories of channels which can be collapsed into one.\n\n            \n            Attribution Devices\n            \n             Desktop and mobile are the only two sources for this data and the duplicates need to be removed and combined into one. \n\n             iv. Data Cleaning and Fixing fields:Fixing Attribution Sources and Devices to create clean columns\n            \n            \n            Attribution Channels\n          \n        \n      \n\n      \n      \n        \n          Data Analysis\n        \n        \n          \n            Shaping Data and Analysis\n            This is a critical stage where cleaned data is being transformed into analysis-ready views. This profiling will enable implementation of RFM segmentation of customers and cohort analysis to understand customer behavior, long-term value and loyalty for the business.\n\n            i. Revenue by Channel and Lifetime Value of customers:Order_count and Average Order value are also calculated for orders attributed to each channel to give a complete picture of contribution of a marketing channel\n\n            \n            Revenue and AOV\n\n            Almost 87% of the total revenue is coming from the top 3 channels and just over 42% of it comes organically, which suggests that the brand and its products are quite popular among customers. YouTube (OLV or CTV channel) as a platform has turned out to be good contributor to the revenue and so has google ads. However, the difference between the 2nd channel and 3rd channel is almost ~15%. Only 5.33% of the total customers have made more than one purchase.\n            \n            \n            LTV by customer\n\n            Life span days for each customer are calculated based on their purchase data, LTV calculation in this case is useless as most of the customers are one-time buyers till date.\n\n            ii. Recency, Frequency and Monetary Segmentation model (RFM):\n            \n            RFM model results\n            \n            Each behavior is categorized into 4 different quartiles. For reference: \n               Recency – How recent the customers made a purchase (lower = more recent)\n               Frequency – How many times did they make a purchase (lower = more active)\n               Monetary – How much do they spend in total with their purchase (lower = high spender)\n               This score across the 3 metrics helps create behavior-based segments like Champions (1-1-1) and At risk customers (3/4, 3/5, 3/4).\n\n            As seen in previous analysis, the business has lot of newly acquired customers so we have high number of customers falling in the (R1F1M1-4) category. Opportunities lies for improvement with customers that are in the (R2F2M1-4) category by analyzing gaps in their purchase experience and sentiment analysis. There are 177 customers in the R3F3 or R4F4 category which are the least profitable and budget allocation to this category should be carefully implemented as these can be no longer interested cohort.\n\n            iii. Cohort Retention Analysis:\n            \n            \n            Purchase behavior\n\n            Cohort Analysis understanding: In simple ways cohort analysis can be thought of grouping your customers from their start date and tracking their behavior over time. \n            RFM: Taking a snapshot of customers TODAY \n            Cohort: Making a movie of their behavior over TIME\n            Types of Cohorts: Time-based – Grouping by when they first purchased (acquisition date)\n            Behavior-based – Group by their first action or characteristic\n           \n           \n            RFM tells you:\n            \n            Who your best customers are RIGHT NOW\n            What their current value is\n           How to segment them for targeted campaigns\n            \n            \n            Cohort Analysis tells you:\n\n            When customers tend to drop off\n            How long different segments stay valuable\n            Whether new cohorts behave better/worse than older ones\n            \n            In this case Cohort analysis has been implemented using time-based data as customers are grouped by their first month of purchase (cohort month) and then tracking how many subsequent months they remain active.\n\n            \n            Cohorts\n\n            The quantiles here are split into 5 equal groups as there are 375 customers and the spending resembles pareto distribution where 20% of the customers are bringing in 80% of the revenue for the business. The difference between the loyalists and low/one-time spenders is that loyalists outspend them by 51% suggesting there is lot of gaps for the business to fill between 1-3 quartiles.\n\n            iv. Top products by Customer segments:\n\n            \n            Top products by segments\n            \n            ShipStream is the top product that has generated the most revenue for 5 out of 7 customer segments which indicates it is a popular product. On average it accounts for 60% of the revenue across the segments. All the New customers have just purchased one product which is ProspectHub and it generates the lowest revenue among all the other top products. In terms of trend, for all the segments the top 2 products generate the majority of the revenue than the remaining in the archetypes.\n          \n        \n      \n\n      \n      \n        \n          Dashboard Presentation & Final recommendations\n        \n        \n          \n              Dashboard Walkthrough Video\n                 \n               \n              \n            \n\n              \n              Final recommendations:\n                Collect session & user journey data on the website to find gaps in the user experience specifically for At churn risk customers\n                Build post-purchase experience stack like email/SMS flows (how-to, reshare UGC, cross-sell etc) to target the first 30 to 60 days and lift the repeat purchase rate from 5.33% to 8-10%. Track the cohort M+1 – M+3 after these efforts for the repeat purchase rate.\n                Set a KPI benchmark to grow Champions’ share of revenue and measure the incremental AOV and revenue as well as order frequency after attempting these efforts. Providing early access to Champions’, limited edition drops and exclusive surprises to top their experience.\n                RFM archetypes can be used to redefine the homepage modules with favorite packs or promotional offers and cross-sell blocks. For example, Champions’ would look at premium bundles whereas one purchase wonders would look at exclusive enticing offers that force them to make the move. At churn risk customers will look for promotional offers\n                In terms of the other products, analyze the products that don’t make large share of revenue to see if they are one-time purchase by nature or if there is potential to improve the product or how they are put on the shelf.\n              \n          \n        \n      \n    \n  \n\n\n\nA/B Testing of Advertising Campaign Results\n\n  \n    \n      \n        \n      \n      \n        \n          A/B Testing analysis of Direct Mail Marketing Campaign\n        \n        \n          Analyzing A/B tests for advertising campaigns to measure effectiveness, optimize ad spend, and identify the most impactful creative and targeting strategies as well as recommending audience strategy based on purchase patterns.\n        \n        \n          \n             GitHub\n          \n          \n             Power BI\n          \n        \n      \n    \n  \n\n  \n    \n      \n      \n        \n          Project Scenario\n        \n        \n          \n            Context and Overview\n            As a junior marketing data analyst for a Marketing agency working for a cruise client, you have received the AB testing \n            results for a direct mail marketing campaign and the client wants to understand if the test generated incremental results as well as any notable changes in the audience purchase patterns and any other noteworthy changes from status quo.\n            \n            \n            Key Objectives\n            \n              Compare overall performance of test group against the control group\n              Identify patterns and/or anomalies by deep diving into performance by variables (creatives, destinations)\n              Analyze audience performance by key metrics identified.\n            \n          \n        \n      \n\n      \n      \n        \n          Business Problem and Stakeholders Questions\n        \n        \n          \n            Business Problem:\n             How can we justify the leadership team of the cruise company to invest in additional direct mail marketing spending towards identified segments to generate incremental bookings and profitable returns which would otherwise have not been generated if we just maintained the status quo.\n\n            Stakeholder Questions\n            \n              Campaigns/audiences that can be scaled up or replicated.\n              Destinations that can be reconsidered/dropped\n              Top audience segments and the targeting strategy for them\n              High and low performing creatives\n              Interaction effect of variables\n              Actionable recommendations for future campaigns\n            \n          \n        \n      \n\n      \n      \n        \n          Data Requirements & Gathering\n        \n        \n          \n            Data source\n            A raw data excel file (AB testing analysis) contains almost 56K records of Test and Control group for the mail marketing campaign. Each record has actual and project conversions and revenue rolled up by Destinations, and it also contains creative, audience segment and campaign details for each record for granular analysis. Overall the data is confirmed to be at perfect aggregate level to perform the necessary analysis and allows us to compare the overall impact of Test group across different markets.\n          \n        \n      \n\n      \n      \n        \n          Data Cleaning & Transformation\n        \n        \n          \n            The raw data of AB test results are at perfect aggregate level, hence no major transformations are required. Orders and revenue are rolled up at destination level. From an analysis perspective, custom field like 'v1' are not important in this case and since Control group is not exposed to any change in variables, the audience segment is blank against those rows. However, we need baseline for the comparison, so we keep the control rows as well. There are Test records which do not have a corresponding 'has hold' record however these are newly tested creatives/campaigns so we keep them for now. The client has set benchmarks for campaigns that are considered statistically significant for analysis - those with at least 25000 mail count\n          \n        \n      \n\n      \n      \n        \n          Data Analysis\n        \n        \n          \n             A high-level summary of the key findings from the A/B test. For detailed metrics, interactive exploration, and final strategic recommendations, please see the complete Dashboard Presentation & Final recommendations.\n\n             High level performance overview\n             \n            High level Control vs Test performance\n            \n             Control group is the one that received the standard mailing and defines the status quo, they are the baseline for comparison.\n             Test group recieves the modified version of mails where one key element is changed (in this case it is the destination, creative or a comnbination of both)\n             The incremental metrics in the above table will be for test group compared to baseline group, as the volume of mails is 82% higher in the test group it influences the incremental numbers strongly.\n            \n            \n            Performance benchmarks\n            \n            \n            Key analysis points\n            \n              The 25K mail volume benchmark removes any campaigns where the hold/control group is not present\n              Kids sail free and Default are the combined group of destinations that are mailed consistently throughout the year and hence their performance is consistent\n              AOV in the cruise industry is expected to be around $3K, because of the way the packages are designed and sold and hence it shouldn't be considered as basis for audience segmentation.\n            \n          \n        \n      \n\n           \n      \n        \n          Dashboard Presentation & Final recommendations\n        \n        \n          \n            \n            \n            \n          \n          Please click on the Power BI dashboard link for the Live Power BI report to\n             play with the data\n        \n      \n      \n     \n   \n \n\n\nBig Data Analysis of Lightcast US Tech job postings\n\n  \n    \n      \n        \n        \n      \n      \n        \n          Lightcast US Tech job postings analysis\n        \n        \n          This project covers the structured analysis of how three of the most competitive and rapidly evolving fields in data and tech \n          (Machine Learning Operations, Data Science and Business Analysts) are demanding candidates to consistently reskill and upskill. The content covered is detailed quantitative analysis of Big data from Lightcast for these positions and the patterns and trends are contextualized by the qualitative analysis through various research papers. \n        \n        \n          \n             GitHub\n          \n        \n      \n    \n  \n\n  \n  \n    \n      \n        \n        Explore the Full Project Website\n        This project has its own dedicated website with the complete case study, including the business context, interactive visualizations, model insights, and final roadmap for my career path in one of these fields.\n        \n          Visit Project Site \n        \n      \n    \n  \n\n\n\nCovid-19 Testing Capacity and Resource Allocation Analysis\n\n  \n    \n      \n        \n      \n      \n        \n          Covid-19 Testing Capacity and Resource Allocation Analysis\n        \n        \n          Analyzing testing capacity data and resource allocation patterns to optimize public health response and ensure efficient distribution of testing resources during the pandemic.\n        \n        \n          \n             GitHub\n          \n          \n             Power BI\n          \n        \n      \n    \n  \n\n  \n    \n      \n        \n          Business Problem and Stakeholders Questions\n        \n        \n          \n            Content about business problem and stakeholder questions will go here...\n          \n        \n      \n\n      \n        \n          Data Requirements & Gathering\n        \n        \n          \n            Content about data requirements and gathering process will go here...\n          \n        \n      \n\n      \n        \n          Data Analysis\n        \n        \n          \n            Content about data analysis methodology and findings will go here...\n          \n        \n      \n\n      \n        \n          Presentation\n        \n        \n          \n            \n              \n              Dashboard/Video Presentation will be embedded here"
  }
]